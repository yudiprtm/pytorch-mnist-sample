{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Our task is simple, recognize handwritten digits. We will use MNIST dataset for this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary library\n",
    "In this tutorial, we are going to use pytorch, the cutting-edge deep learning framework to complete our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336550d4e921499ba3e66ff94e3a1e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST\\raw\\train-images-idx3-ubyte.gz to dataset/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5da3975ad74dc8861cd0c178c5e386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST\\raw\\train-labels-idx1-ubyte.gz to dataset/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61679fd4a9154b25b5177678fa94c880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST\\raw\\t10k-images-idx3-ubyte.gz to dataset/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22bc6f91c7a45799b51c9241a91acf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to dataset/MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create dataloader, in PyTorch, we feed the trainer data with use of dataloader\n",
    "## We create dataloader with dataset from torchvision, \n",
    "## and we dont have to download it seperately, all automatically done\n",
    "\n",
    "# Define batch size, batch size is how much data you feed for training in one iteration\n",
    "batch_size_train = 64 # We use a small batch size here for training\n",
    "batch_size_test = 1024 #\n",
    "\n",
    "# define how image transformed\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])\n",
    "#image datasets\n",
    "train_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                           train=True, \n",
    "                                           download=True,\n",
    "                                           transform=image_transform)\n",
    "test_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                          train=False, \n",
    "                                          download=True,\n",
    "                                          transform=image_transform)\n",
    "#data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size_test, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor(0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import library\n",
    "import matplotlib.pyplot as plt\n",
    "# We can check the dataloader\n",
    "_, (example_datas, labels) = next(enumerate(test_loader))\n",
    "sample = example_datas[0][0]\n",
    "# show the data\n",
    "plt.imshow(sample, cmap='gray', interpolation='none')\n",
    "print(\"Label: \"+ str(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we can start to build our CNN model\n",
    "## We first import the pytorch nn module and optimizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "## Then define the model class\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        #input channel 1, output channel 10\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, stride=1)\n",
    "        #input channel 10, output channel 20\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=1)\n",
    "        #dropout layer\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        #fully connected layer\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_drop(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model and optimizer\n",
    "learning_rate = 0.15\n",
    "momentum = 0.000025\n",
    "device = \"cpu\"\n",
    "model = CNN().to(device) #using cpu here\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "##define train function\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10000):\n",
    "    model.train()\n",
    "    tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    counter = 0\n",
    "    for batch_idx, (data, target) in enumerate(tk0):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        counter += 1\n",
    "        tk0.set_postfix(loss=(loss.item()*data.size(0) / (counter * train_loader.batch_size)))\n",
    "##define test function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_992/1895196525.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tk0 = tqdm(train_loader, total=int(len(train_loader)))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d637a15ba9244e44a6e79a3c0e88f2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_992/4020281764.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1754, Accuracy: 9490/10000 (95%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce0270b4b2645d0a8e7b6579200edbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1367, Accuracy: 9603/10000 (96%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dd536f62014bdaad59b325696a9550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1151, Accuracy: 9669/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 3\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch-summary in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (1.4.5)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\Program Files\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 10, 24, 24]          260\n",
      "├─Conv2d: 1-2                            [-1, 20, 8, 8]            5,020\n",
      "├─Dropout2d: 1-3                         [-1, 20, 8, 8]            --\n",
      "├─Linear: 1-4                            [-1, 50]                  16,050\n",
      "├─Linear: 1-5                            [-1, 10]                  510\n",
      "==========================================================================================\n",
      "Total params: 21,840\n",
      "Trainable params: 21,840\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.48\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.14\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp/ipykernel_992/4020281764.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 10, 24, 24]          260\n",
       "├─Conv2d: 1-2                            [-1, 20, 8, 8]            5,020\n",
       "├─Dropout2d: 1-3                         [-1, 20, 8, 8]            --\n",
       "├─Linear: 1-4                            [-1, 50]                  16,050\n",
       "├─Linear: 1-5                            [-1, 10]                  510\n",
       "==========================================================================================\n",
       "Total params: 21,840\n",
       "Trainable params: 21,840\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.08\n",
       "Estimated Total Size (MB): 0.14\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (1, 28, 28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
